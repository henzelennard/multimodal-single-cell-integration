{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab6e3231-9e16-49d3-bd9b-5e884f56695d",
   "metadata": {},
   "source": [
    "The size of the dataset was one issue I ran into while analyzing the competition data. Due to a memory shortage, it was difficult to compute several of the dataset's important properties. This notebook's objective is to compute a few features chunkwise and store those results as its output. These properties can then be included into models or used for EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b37a9e3b-58e8-4c4a-a213-12f4cd3ddfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# data imports\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab729566-bd65-4778-bac2-ee977d09c3a0",
   "metadata": {},
   "source": [
    "We will use the GPU to speed up calculations by a factor of almost 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5565dbe0-459c-49ec-908a-4b7d28bb2559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/len/Data/Multimodal/train_multi_targets_T.h5'),\n",
       " PosixPath('/home/len/Data/Multimodal/test_multi_inputs_T.h5'),\n",
       " PosixPath('/home/len/Data/Multimodal/train_multi_inputs_T.h5'),\n",
       " PosixPath('/home/len/Data/Multimodal/train_cite_inputs_T.h5'),\n",
       " PosixPath('/home/len/Data/Multimodal/test_cite_inputs_T.h5'),\n",
       " PosixPath('/home/len/Data/Multimodal/train_cite_targets_T.h5')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "data_dir = os.environ['DATA_DIR']\n",
    "\n",
    "f_list = list(Path(data_dir).glob('./*T.h5'))\n",
    "f_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff3a2366-6781-40f7-99a1-286f480819d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "# field should be a tensor only containing zeros and ones\n",
    "# this computes n tuples in the dataset\n",
    "def get_naive_n_tuples(field, max_depth):\n",
    "    naive_num_tuples = []\n",
    "    #append one-tuples (=number of elements) to naive tuple count\n",
    "    naive_num_tuples.append(field.sum(dim=1))\n",
    "    \n",
    "    # iterativeley calculate a new field. 1 in the field means, beginning of n tuple\n",
    "    prev_field = field.clone().detach()\n",
    "    for i in range(max_depth):\n",
    "        prev_field = prev_field[:,:-1] * field[:,i+1:]\n",
    "        naive_num_tuples.append(prev_field.sum(dim=1))\n",
    "            \n",
    "    return naive_num_tuples\n",
    "\n",
    "# get_naive_n_tuples adds 2 2-tuple counts for every 3-tuple and so on\n",
    "# this function removes those duplicates\n",
    "def clean_naive_n_tuples(n_tuples):\n",
    "    reversed_n_tuples = list(reversed(n_tuples))\n",
    "    reverse_clean_tuples = []\n",
    "    \n",
    "    for i in range(len(reversed_n_tuples)):\n",
    "        reverse_clean_tuples.append(reversed_n_tuples[i].clone().detach())\n",
    "        \n",
    "        # correct number\n",
    "        for j in range(i):\n",
    "            reverse_clean_tuples[i] -= (i+1-j) * reverse_clean_tuples[j] \n",
    "        \n",
    "    return list(reversed(reverse_clean_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ccad5b6-09c2-4c28-a6dd-911e39a42095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### OPERATING ON: /home/len/Data/Multimodal/train_multi_targets_T.h5 ###\n",
      "### OPERATING ON: /home/len/Data/Multimodal/test_multi_inputs_T.h5 ###\n",
      "### OPERATING ON: /home/len/Data/Multimodal/train_multi_inputs_T.h5 ###\n",
      "### OPERATING ON: /home/len/Data/Multimodal/train_cite_inputs_T.h5 ###\n",
      "### OPERATING ON: /home/len/Data/Multimodal/test_cite_inputs_T.h5 ###\n",
      "### OPERATING ON: /home/len/Data/Multimodal/train_cite_targets_T.h5 ###\n"
     ]
    }
   ],
   "source": [
    "for f_name in f_list:\n",
    "    print(f\"### OPERATING ON: {f_name} ###\")\n",
    "    \n",
    "    # Get properties of dataset\n",
    "    with h5py.File(f_name) as f:\n",
    "        cell_names = [a.decode('utf-8') for a in f['cells']]\n",
    "        feature_names = [a.decode('utf-8') for a in f[\"features\"]]\n",
    "    NUM_FEATURES = len(feature_names)\n",
    "    NUM_CELLS = len(cell_names)\n",
    "    CHUNK_SIZE = int(200_000_000 / (NUM_CELLS))\n",
    "\n",
    "    # the features we want to calculate for every cell\n",
    "    target_features = {\n",
    "        'count_non_zero': torch.zeros(NUM_FEATURES, dtype=torch.int32, device=device),\n",
    "        'max_value': torch.zeros(NUM_FEATURES, dtype=torch.float64, device=device),\n",
    "        'min_value': torch.zeros(NUM_FEATURES, dtype=torch.float64, device=device),\n",
    "        'sum_values': torch.zeros(NUM_FEATURES, dtype=torch.float64, device=device),\n",
    "        'mean_non_zero': torch.zeros(NUM_FEATURES, dtype=torch.float64, device=device),\n",
    "\n",
    "        # We would like to calc std_dev but currently pytorch does not support it\n",
    "        #'std_dev_non_zero': torch.zeros(NUM_FEATURES, dtype=torch.float64, device=device),\n",
    "\n",
    "        # number of consecutive non-zero elements / n_tupels\n",
    "        '1_tups': torch.zeros(NUM_FEATURES, dtype=torch.int32, device=device),\n",
    "        '2_tups': torch.zeros(NUM_FEATURES, dtype=torch.int32, device=device),\n",
    "        '3_tups': torch.zeros(NUM_FEATURES, dtype=torch.int32, device=device),\n",
    "        '4_tups': torch.zeros(NUM_FEATURES, dtype=torch.int32, device=device),\n",
    "        '5_tups': torch.zeros(NUM_FEATURES, dtype=torch.int32, device=device),\n",
    "        '6_tups': torch.zeros(NUM_FEATURES, dtype=torch.int32, device=device),\n",
    "        '7_tups': torch.zeros(NUM_FEATURES, dtype=torch.int32, device=device),\n",
    "        '8_tups': torch.zeros(NUM_FEATURES, dtype=torch.int32, device=device),\n",
    "        '9_tups': torch.zeros(NUM_FEATURES, dtype=torch.int32, device=device),\n",
    "        '10_tups': torch.zeros(NUM_FEATURES, dtype=torch.int32, device=device),\n",
    "        '11_tups': torch.zeros(NUM_FEATURES, dtype=torch.int32, device=device),\n",
    "        '12_tups': torch.zeros(NUM_FEATURES, dtype=torch.int32, device=device),\n",
    "        '13_tups': torch.zeros(NUM_FEATURES, dtype=torch.int32, device=device),\n",
    "        '14_tups': torch.zeros(NUM_FEATURES, dtype=torch.int32, device=device),\n",
    "        '15_tups': torch.zeros(NUM_FEATURES, dtype=torch.int32, device=device),\n",
    "        '16_tups': torch.zeros(NUM_FEATURES, dtype=torch.int32, device=device),\n",
    "    }\n",
    "    \n",
    "    # iterate over dataset calculating features\n",
    "    for i in range(int(np.ceil(NUM_FEATURES/CHUNK_SIZE))):\n",
    "        ###### SPECIFICATION WHAT DATA TO LOAD AND LOADING OF DATA ONTO DEVICE (CPU OR CUDA) #########\n",
    "        S_INDEX = i * CHUNK_SIZE\n",
    "        E_INDEX = (i+1) * CHUNK_SIZE\n",
    "\n",
    "        # load data and send to torch device\n",
    "        with h5py.File(f_name) as f:\n",
    "            data = torch.tensor(f['values'][S_INDEX : E_INDEX], device=device)\n",
    "\n",
    "            \n",
    "        gc.collect()\n",
    "        ##### CALCULATION OF FEATURES #####\n",
    "        target_features['count_non_zero'][S_INDEX:E_INDEX] = data.gt(0).sum(dim=1)\n",
    "        target_features['max_value'][S_INDEX:E_INDEX] = data.max(dim=1)[0]\n",
    "        target_features['min_value'][S_INDEX:E_INDEX] = data.min(dim=1)[0]\n",
    "        target_features['sum_values'][S_INDEX:E_INDEX] = data.sum(dim=1)\n",
    "\n",
    "\n",
    "        # set zero values to nan (this helps in some computations \n",
    "        # e. g. when computation mean of non zero values)\n",
    "        data[torch.eq(data, 0)] = torch.nan\n",
    "\n",
    "        target_features['mean_non_zero'][S_INDEX:E_INDEX] = data.nanmean(dim=1)\n",
    "\n",
    "        # missing implementation in pytorch (is on their todo)\n",
    "        #target_features['std_dev_non_zero'][S_INDEX:E_INDEX] = data.nanstd(dim=1)\n",
    "\n",
    "        naive_n_tuples = get_naive_n_tuples(data.gt(0), 15)\n",
    "        clean_tuples = clean_naive_n_tuples(naive_n_tuples)\n",
    "\n",
    "        target_features['1_tups'][S_INDEX:E_INDEX] = clean_tuples[0]\n",
    "        target_features['2_tups'][S_INDEX:E_INDEX] = clean_tuples[1]\n",
    "        target_features['3_tups'][S_INDEX:E_INDEX] = clean_tuples[2]\n",
    "        target_features['4_tups'][S_INDEX:E_INDEX] = clean_tuples[3]\n",
    "        target_features['5_tups'][S_INDEX:E_INDEX] = clean_tuples[4]\n",
    "        target_features['6_tups'][S_INDEX:E_INDEX] = clean_tuples[5]\n",
    "        target_features['7_tups'][S_INDEX:E_INDEX] = clean_tuples[6]\n",
    "        target_features['8_tups'][S_INDEX:E_INDEX] = clean_tuples[7]\n",
    "        target_features['9_tups'][S_INDEX:E_INDEX] = clean_tuples[8]\n",
    "        target_features['10_tups'][S_INDEX:E_INDEX] = clean_tuples[9]\n",
    "        target_features['11_tups'][S_INDEX:E_INDEX] = clean_tuples[10]\n",
    "        target_features['12_tups'][S_INDEX:E_INDEX] = clean_tuples[11]\n",
    "        target_features['13_tups'][S_INDEX:E_INDEX] = clean_tuples[12]\n",
    "        target_features['14_tups'][S_INDEX:E_INDEX] = clean_tuples[13]\n",
    "        target_features['15_tups'][S_INDEX:E_INDEX] = clean_tuples[14]\n",
    "        target_features['16_tups'][S_INDEX:E_INDEX] = clean_tuples[15]\n",
    "        # target_features['feature'][S_INDEX:E_INDEX] = \n",
    "    \n",
    "    # calculations done, define index, build dataframe and safe as csv\n",
    "    target_features_cpu = {key:value.cpu() for key, value in target_features.items()}\n",
    "    df = pd.DataFrame(data=target_features_cpu, index=feature_names)\n",
    "    df.to_csv(data_dir + f_name.stem + '_feature_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:msci]",
   "language": "python",
   "name": "conda-env-msci-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
