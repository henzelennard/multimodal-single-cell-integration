{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08a333a7-7c43-41e3-9245-bf3b010dbf10",
   "metadata": {},
   "source": [
    "# DATA TRANSFORMATION SCRIPT\n",
    "The Data we have is arranged by cells. That means the features of a cell are consecutive in memory. If we want to look at multiple features of different cells this results in really long loading times. Since memory on hard drive is no huge concern we will create transposed datasets that allow fast access of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56fbd065-3dc5-4e7c-93ca-49d128837dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# libs for reading and writing hdf5 files\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "\n",
    "# libs for env and file handling\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ceil function from numpy\n",
    "from numpy import ceil as ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c45db9cb-577b-4d79-a515-2e782bcd4f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_multi_inputs',\n",
       " 'train_cite_targets',\n",
       " 'train_multi_inputs',\n",
       " 'train_cite_inputs',\n",
       " 'train_multi_targets',\n",
       " 'test_cite_inputs']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data dir from the environment and create a list of files that should be transposed\n",
    "data_path = Path(os.environ['DATA_DIR'])\n",
    "f_names = [a.stem for a in data_path.glob('./*[!T].h5')]\n",
    "f_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcff77ea-12cf-43c7-93fe-b7ddeeca8e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on File: test_multi_inputs\n",
      "Input File: /home/len/Data/Multimodal/test_multi_inputs.h5\n",
      "Output File: /home/len/Data/Multimodal/test_multi_inputs_T.h5\n",
      "Starting Data Copy. Total iterations: 1\n",
      "\n",
      "Working on File: train_cite_targets\n",
      "Input File: /home/len/Data/Multimodal/train_cite_targets.h5\n",
      "Output File: /home/len/Data/Multimodal/train_cite_targets_T.h5\n",
      "Starting Data Copy. Total iterations: 1\n",
      "\n",
      "Working on File: train_multi_inputs\n",
      "Input File: /home/len/Data/Multimodal/train_multi_inputs.h5\n",
      "Output File: /home/len/Data/Multimodal/train_multi_inputs_T.h5\n",
      "Starting Data Copy. Total iterations: 2\n",
      "\n",
      "Working on File: train_cite_inputs\n",
      "Input File: /home/len/Data/Multimodal/train_cite_inputs.h5\n",
      "Output File: /home/len/Data/Multimodal/train_cite_inputs_T.h5\n",
      "Starting Data Copy. Total iterations: 1\n",
      "\n",
      "Working on File: train_multi_targets\n",
      "Input File: /home/len/Data/Multimodal/train_multi_targets.h5\n",
      "Output File: /home/len/Data/Multimodal/train_multi_targets_T.h5\n",
      "Starting Data Copy. Total iterations: 1\n",
      "\n",
      "Working on File: test_cite_inputs\n",
      "Input File: /home/len/Data/Multimodal/test_cite_inputs.h5\n",
      "Output File: /home/len/Data/Multimodal/test_cite_inputs_T.h5\n",
      "Starting Data Copy. Total iterations: 1\n"
     ]
    }
   ],
   "source": [
    "# iterate over files to be transposed and create transposed counterpart\n",
    "for f_name in f_names:\n",
    "    print(f\"\\nWorking on File: {f_name}\")\n",
    "    from_path = Path(data_path / (f_name + '.h5'))\n",
    "    to_path = Path(data_path / (f_name + '_T.h5'))\n",
    "    print(f\"Input File: {from_path}\")\n",
    "    print(f\"Output File: {to_path}\")\n",
    "    \n",
    "    # copy index and columnnames\n",
    "    with h5py.File(from_path, 'r') as from_file:\n",
    "        cells_dset = from_file[f_name + '/axis1']\n",
    "        from_cells = {\n",
    "            'data': cells_dset[:],\n",
    "            'dtype': cells_dset.dtype,\n",
    "            'shape':cells_dset.shape,\n",
    "        }\n",
    "        features_dset = from_file[f_name + '/axis0']\n",
    "        from_features = {\n",
    "            'data': features_dset[:],\n",
    "            'dtype': features_dset.dtype,\n",
    "            'shape':features_dset.shape,\n",
    "        }\n",
    "        \n",
    "    num_cells = from_cells['shape'][0]\n",
    "    num_features = from_features['shape'][0]\n",
    "\n",
    "    with h5py.File(to_path, 'w') as to_file:\n",
    "        to_cells = to_file.create_dataset('cells', from_cells['shape'], dtype=from_cells['dtype'], compression=\"gzip\")\n",
    "        to_features = to_file.create_dataset('features', from_features['shape'], dtype=from_features['dtype'], compression=\"gzip\")\n",
    "        \n",
    "        to_cells[:] = from_cells['data']\n",
    "        to_features[:] = from_features['data']\n",
    "    \n",
    "    \n",
    "    # iterate over old dataset\n",
    "    # we have 55GB ram availabl for the transformations, adjust to your ram\n",
    "    avail_ram_in_GB = 55\n",
    "    avail_ram_bytes = avail_ram_in_GB * 2**(3*10)\n",
    "    max_float_in_mem = int(avail_ram_bytes / 4)\n",
    "    batchsize = int(ceil(max_float_in_mem / num_features))\n",
    "    iterations = int(ceil(num_cells / batchsize))\n",
    "\n",
    "    # create dataset\n",
    "    with h5py.File(from_path, 'r') as from_file, h5py.File(to_path, 'r+') as to_file:\n",
    "        to_file.create_dataset('values', \n",
    "                               from_file[f_name + '/block0_values'].shape[::-1], \n",
    "                               dtype=from_file[f_name + '/block0_values'].dtype,\n",
    "                               compression=\"gzip\"\n",
    "        )\n",
    "    \n",
    "    print(f\"Starting Data Copy. Total iterations: {iterations}\")\n",
    "    for i in range(iterations):\n",
    "        # create dataset objects\n",
    "        s_index = i * batchsize\n",
    "        e_index = (i+1) * batchsize\n",
    "    \n",
    "        with h5py.File(from_path, 'r') as from_file, h5py.File(to_path, 'r+') as to_file:\n",
    "            to_file['/values'][i*batchsize:(i+1)*batchsize, :] = from_file[f_name + '/block0_values'][:, i*batchsize: (i+1)*batchsize].T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:msci]",
   "language": "python",
   "name": "conda-env-msci-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
